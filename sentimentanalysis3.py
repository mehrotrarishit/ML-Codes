# -*- coding: utf-8 -*-
"""SentimentAnalysis3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L15-4VXJAh6XIR8ERiuP7fjfPLIY-5oH
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import nltk

train=pd.read_csv("Dataset_Kaggle.csv")
test=pd.read_csv("Test_upload.csv")

train

train.isnull().sum()

train=train.dropna(subset=['Comments'])

train.isnull().sum()

train=train.dropna(subset=['Positive'])

train.isnull().sum()

train

Id=train.Id

train

nltk.download('stopwords')

from nltk.corpus import stopwords

stop_words=set(stopwords.words('english'))

from nltk.tokenize import word_tokenize

from nltk.tokenize.treebank import TreebankWordDetokenizer

!pip install transformers

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

Model=f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer=AutoTokenizer.from_pretrained(Model)
model=AutoModelForSequenceClassification.from_pretrained(Model)

nltk.download('punkt')

res={}
for i,row in train.iterrows():
  text=row['Comments']
  tokenize_words=word_tokenize(text)
  t_w_without_stop_words=[]
  for word in tokenize_words:
    if word not in stop_words:
      t_w_without_stop_words.append(word)
  text1=' '.join(t_w_without_stop_words)
  encoded_text=tokenizer(text1,return_tensors='pt')
  output_text=model(**encoded_text)
  scores=output_text[0][0].detach().numpy()
  scores=softmax(scores)
  myid=row['Id']
  res[myid]=scores

roberta=pd.DataFrame(res).T

roberta

roberta.columns=['neg','neu','pos']

roberta

roberta.isnull().sum()

train1_x=roberta
train1_y=train.Positive

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(train1_x,train1_y,test_size=0.2)

rf=RandomForestClassifier(n_estimators=100)
rf.fit(train1_x,train1_y)

rf.score(x_test,y_test)

test

test.isnull().sum()

res={}
for i,row in test.iterrows():
  text=row['Comment']
  tokenize_words=word_tokenize(text)
  t_w_without_stop_words=[]
  for word in tokenize_words:
    if word not in stop_words:
      t_w_without_stop_words.append(word)
  text1=' '.join(t_w_without_stop_words)
  encoded_text=tokenizer(text1,return_tensors='pt')
  print(encoded_text)
  output_text=model(**encoded_text)
  print(output_text)
  scores=output_text[0][0].detach().numpy()
  print(scores)
  scores=softmax(scores)
  myid=row['Id']
  res[myid]=scores

res

test_roberta=pd.DataFrame(res).T

test_roberta.columns=['neg','neu','pos']

test_roberta

predict_score=rf.predict(test_roberta)

predict_score1=predict_score.astype(int)

predict_score1.reshape(150,1)

output=pd.DataFrame({"Id":test.Id,"Positive":predict_score1})

output.to_csv("Sentiment Analysis Submission.csv",index=False)

